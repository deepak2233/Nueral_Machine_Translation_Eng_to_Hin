{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UhoLQDkGgdx8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip '/project/code/deepak/Pytorch_Tutorial/Machine_T/NEWS2018_DATASET_04.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import xml.etree.ElementTree as Xet\n",
    "import pandas as pd\n",
    "\n",
    "cols = [\"SourceName\", \"TargetName\"]\n",
    "rows = []\n",
    "\n",
    "# Parsing the XML file\n",
    "xmlparse = Xet.parse('/project/datasets/code/deepak/Pytorch_Tutorial/Machine_T/NEWS2018_DATASET_04/NEWS2018_M-EnHi_train.xml')\n",
    "root = xmlparse.getroot()\n",
    "for i in root:\n",
    "    SourceName = i.find(\"SourceName\").text\n",
    "    TargetName = i.find(\"TargetName\").text\n",
    "\n",
    "\n",
    "    rows.append({\"SourceName\": SourceName,\n",
    "                 \"TargetName\": TargetName,\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "# Writing dataframe to csv\n",
    "df.to_csv('Output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import xml.etree.ElementTree as Xet\n",
    "import pandas as pd\n",
    "\n",
    "cols = [\"SourceName\", \"TargetName\"]\n",
    "rows = []\n",
    "\n",
    "# Parsing the XML file\n",
    "xmlparse = Xet.parse('/project/datasets/code/deepak/Pytorch_Tutorial/Machine_T/NEWS2018_DATASET_04/NEWS2018_M-EnHi_dev.xml')\n",
    "root = xmlparse.getroot()\n",
    "for i in root:\n",
    "    SourceName = i.find(\"SourceName\").text\n",
    "    TargetName = i.find(\"TargetName\").text\n",
    "\n",
    "\n",
    "    rows.append({\"SourceName\": SourceName,\n",
    "                 \"TargetName\": TargetName,\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "# Writing dataframe to csv\n",
    "df.to_csv('Validation_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQNeTtWZlwUc",
    "outputId": "0be1428c-298b-4dc0-edb5-f3bd1da7dd1c"
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    return w\n",
    "\n",
    "def hindi_preprocess_sentence(w):\n",
    "    w = w.rstrip().strip()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path):\n",
    "    lines=pd.read_csv(path,index_col =[0])\n",
    "    lines=lines.dropna()\n",
    "    \n",
    "    en = []\n",
    "    hd = []\n",
    "    for i, j in zip(lines['SourceName'], lines['TargetName']):\n",
    "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
    "        en_1.append('<end>')\n",
    "        en_1.insert(0, '<start>')\n",
    "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
    "        hd_1.append('<end>')\n",
    "        hd_1.insert(0, '<start>')\n",
    "        en.append(en_1)\n",
    "        hd.append(hd_1)\n",
    "    return hd, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/project/datasets/code/deepak/Pytorch_Tutorial/Machine_T/output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_path = '/project/datasets/code/deepak/Pytorch_Tutorial/Machine_T/Validation_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(val_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = pd.concat([train_data,val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data.to_csv('whole_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/project/datasets/code/deepak/Pytorch_Tutorial/Machine_T/whole_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    targ_lang, inp_lang = create_dataset(data_path)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(train_data_path)\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11149 11149 2788 2788\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "11631 ----> shivlal\n",
      "1785 ----> yadav\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "11247 ----> शिवलाल\n",
      "2058 ----> यादव\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "    \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "    return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#   print(type(mask))\n",
    "  loss_ *= mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    # Teacher forcing\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))      \n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.5518\n",
      "Epoch 1 Batch 100 Loss 0.6290\n",
      "Epoch 1 Loss 0.5952\n",
      "Time taken for 1 epoch 4.58247709274292 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.5508\n",
      "Epoch 2 Batch 100 Loss 0.5628\n",
      "Epoch 2 Loss 0.5711\n",
      "Time taken for 1 epoch 4.706860780715942 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.5245\n",
      "Epoch 3 Batch 100 Loss 0.5545\n",
      "Epoch 3 Loss 0.5503\n",
      "Time taken for 1 epoch 4.525805711746216 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.4945\n",
      "Epoch 4 Batch 100 Loss 0.5094\n",
      "Epoch 4 Loss 0.5254\n",
      "Time taken for 1 epoch 4.831615447998047 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.4839\n",
      "Epoch 5 Batch 100 Loss 0.4852\n",
      "Epoch 5 Loss 0.5004\n",
      "Time taken for 1 epoch 4.548644781112671 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.4165\n",
      "Epoch 6 Batch 100 Loss 0.4777\n",
      "Epoch 6 Loss 0.4784\n",
      "Time taken for 1 epoch 4.667583703994751 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.4188\n",
      "Epoch 7 Batch 100 Loss 0.4551\n",
      "Epoch 7 Loss 0.4589\n",
      "Time taken for 1 epoch 4.584505796432495 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.4056\n",
      "Epoch 8 Batch 100 Loss 0.4243\n",
      "Epoch 8 Loss 0.4412\n",
      "Time taken for 1 epoch 4.650224924087524 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.3876\n",
      "Epoch 9 Batch 100 Loss 0.4427\n",
      "Epoch 9 Loss 0.4232\n",
      "Time taken for 1 epoch 4.58628511428833 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.3576\n",
      "Epoch 10 Batch 100 Loss 0.4208\n",
      "Epoch 10 Loss 0.4044\n",
      "Time taken for 1 epoch 4.665889739990234 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.3407\n",
      "Epoch 11 Batch 100 Loss 0.4156\n",
      "Epoch 11 Loss 0.3909\n",
      "Time taken for 1 epoch 4.547861337661743 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.3169\n",
      "Epoch 12 Batch 100 Loss 0.3850\n",
      "Epoch 12 Loss 0.3735\n",
      "Time taken for 1 epoch 4.6603734493255615 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.2980\n",
      "Epoch 13 Batch 100 Loss 0.3602\n",
      "Epoch 13 Loss 0.3534\n",
      "Time taken for 1 epoch 4.552171945571899 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.2860\n",
      "Epoch 14 Batch 100 Loss 0.3457\n",
      "Epoch 14 Loss 0.3367\n",
      "Time taken for 1 epoch 4.735379219055176 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.2866\n",
      "Epoch 15 Batch 100 Loss 0.3262\n",
      "Epoch 15 Loss 0.3220\n",
      "Time taken for 1 epoch 4.560868501663208 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.2858\n",
      "Epoch 16 Batch 100 Loss 0.3108\n",
      "Epoch 16 Loss 0.3090\n",
      "Time taken for 1 epoch 4.8489089012146 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.2492\n",
      "Epoch 17 Batch 100 Loss 0.3116\n",
      "Epoch 17 Loss 0.2945\n",
      "Time taken for 1 epoch 4.48630428314209 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.2313\n",
      "Epoch 18 Batch 100 Loss 0.2951\n",
      "Epoch 18 Loss 0.2793\n",
      "Time taken for 1 epoch 4.701288938522339 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.2184\n",
      "Epoch 19 Batch 100 Loss 0.2728\n",
      "Epoch 19 Loss 0.2657\n",
      "Time taken for 1 epoch 4.613660573959351 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.2176\n",
      "Epoch 20 Batch 100 Loss 0.2486\n",
      "Epoch 20 Loss 0.2546\n",
      "Time taken for 1 epoch 4.641607046127319 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.1989\n",
      "Epoch 21 Batch 100 Loss 0.2499\n",
      "Epoch 21 Loss 0.2412\n",
      "Time taken for 1 epoch 4.55168080329895 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.1949\n",
      "Epoch 22 Batch 100 Loss 0.2209\n",
      "Epoch 22 Loss 0.2277\n",
      "Time taken for 1 epoch 4.8889479637146 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.1888\n",
      "Epoch 23 Batch 100 Loss 0.2227\n",
      "Epoch 23 Loss 0.2160\n",
      "Time taken for 1 epoch 4.577575922012329 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.1658\n",
      "Epoch 24 Batch 100 Loss 0.2167\n",
      "Epoch 24 Loss 0.2045\n",
      "Time taken for 1 epoch 4.698883295059204 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.1545\n",
      "Epoch 25 Batch 100 Loss 0.2126\n",
      "Epoch 25 Loss 0.1950\n",
      "Time taken for 1 epoch 4.597531795501709 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.1560\n",
      "Epoch 26 Batch 100 Loss 0.1815\n",
      "Epoch 26 Loss 0.1861\n",
      "Time taken for 1 epoch 4.735693693161011 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.1362\n",
      "Epoch 27 Batch 100 Loss 0.1809\n",
      "Epoch 27 Loss 0.1791\n",
      "Time taken for 1 epoch 4.522030830383301 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.1164\n",
      "Epoch 28 Batch 100 Loss 0.1763\n",
      "Epoch 28 Loss 0.1640\n",
      "Time taken for 1 epoch 4.675167083740234 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.1154\n",
      "Epoch 29 Batch 100 Loss 0.1565\n",
      "Epoch 29 Loss 0.1491\n",
      "Time taken for 1 epoch 4.6900951862335205 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.1040\n",
      "Epoch 30 Batch 100 Loss 0.1460\n",
      "Epoch 30 Loss 0.1378\n",
      "Time taken for 1 epoch 4.718680381774902 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0926\n",
      "Epoch 31 Batch 100 Loss 0.1265\n",
      "Epoch 31 Loss 0.1274\n",
      "Time taken for 1 epoch 4.617625713348389 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0950\n",
      "Epoch 32 Batch 100 Loss 0.1335\n",
      "Epoch 32 Loss 0.1189\n",
      "Time taken for 1 epoch 4.649044990539551 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0901\n",
      "Epoch 33 Batch 100 Loss 0.1099\n",
      "Epoch 33 Loss 0.1113\n",
      "Time taken for 1 epoch 4.577261686325073 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0797\n",
      "Epoch 34 Batch 100 Loss 0.1144\n",
      "Epoch 34 Loss 0.1043\n",
      "Time taken for 1 epoch 4.693660020828247 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0852\n",
      "Epoch 35 Batch 100 Loss 0.1053\n",
      "Epoch 35 Loss 0.1001\n",
      "Time taken for 1 epoch 4.514552116394043 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0699\n",
      "Epoch 36 Batch 100 Loss 0.0956\n",
      "Epoch 36 Loss 0.0940\n",
      "Time taken for 1 epoch 4.683630704879761 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0675\n",
      "Epoch 37 Batch 100 Loss 0.0917\n",
      "Epoch 37 Loss 0.0879\n",
      "Time taken for 1 epoch 4.564786195755005 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0741\n",
      "Epoch 38 Batch 100 Loss 0.0886\n",
      "Epoch 38 Loss 0.0819\n",
      "Time taken for 1 epoch 4.714160203933716 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0548\n",
      "Epoch 39 Batch 100 Loss 0.0733\n",
      "Epoch 39 Loss 0.0737\n",
      "Time taken for 1 epoch 4.516503572463989 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0441\n",
      "Epoch 40 Batch 100 Loss 0.0675\n",
      "Epoch 40 Loss 0.0654\n",
      "Time taken for 1 epoch 4.729677677154541 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0455\n",
      "Epoch 41 Batch 100 Loss 0.0546\n",
      "Epoch 41 Loss 0.0564\n",
      "Time taken for 1 epoch 4.50963830947876 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0362\n",
      "Epoch 42 Batch 100 Loss 0.0433\n",
      "Epoch 42 Loss 0.0490\n",
      "Time taken for 1 epoch 4.863147020339966 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0349\n",
      "Epoch 43 Batch 100 Loss 0.0527\n",
      "Epoch 43 Loss 0.0448\n",
      "Time taken for 1 epoch 4.611131429672241 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0300\n",
      "Epoch 44 Batch 100 Loss 0.0429\n",
      "Epoch 44 Loss 0.0416\n",
      "Time taken for 1 epoch 4.6013503074646 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0273\n",
      "Epoch 45 Batch 100 Loss 0.0507\n",
      "Epoch 45 Loss 0.0388\n",
      "Time taken for 1 epoch 4.599229335784912 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0357\n",
      "Epoch 46 Batch 100 Loss 0.0345\n",
      "Epoch 46 Loss 0.0384\n",
      "Time taken for 1 epoch 4.624568223953247 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0325\n",
      "Epoch 47 Batch 100 Loss 0.0415\n",
      "Epoch 47 Loss 0.0392\n",
      "Time taken for 1 epoch 4.530842065811157 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0240\n",
      "Epoch 48 Batch 100 Loss 0.0378\n",
      "Epoch 48 Loss 0.0399\n",
      "Time taken for 1 epoch 4.641456604003906 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0245\n",
      "Epoch 49 Batch 100 Loss 0.0480\n",
      "Epoch 49 Loss 0.0430\n",
      "Time taken for 1 epoch 4.55193567276001 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0260\n",
      "Epoch 50 Batch 100 Loss 0.0361\n",
      "Epoch 50 Loss 0.0414\n",
      "Time taken for 1 epoch 4.702320098876953 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0216\n",
      "Epoch 51 Batch 100 Loss 0.0293\n",
      "Epoch 51 Loss 0.0349\n",
      "Time taken for 1 epoch 4.501803636550903 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0241\n",
      "Epoch 52 Batch 100 Loss 0.0247\n",
      "Epoch 52 Loss 0.0313\n",
      "Time taken for 1 epoch 4.698333501815796 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0249\n",
      "Epoch 53 Batch 100 Loss 0.0276\n",
      "Epoch 53 Loss 0.0265\n",
      "Time taken for 1 epoch 4.550107002258301 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.0181\n",
      "Epoch 54 Batch 100 Loss 0.0212\n",
      "Epoch 54 Loss 0.0209\n",
      "Time taken for 1 epoch 4.732931852340698 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0132\n",
      "Epoch 55 Batch 100 Loss 0.0137\n",
      "Epoch 55 Loss 0.0166\n",
      "Time taken for 1 epoch 4.81050443649292 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0070\n",
      "Epoch 56 Batch 100 Loss 0.0147\n",
      "Epoch 56 Loss 0.0127\n",
      "Time taken for 1 epoch 4.7122087478637695 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0066\n",
      "Epoch 57 Batch 100 Loss 0.0106\n",
      "Epoch 57 Loss 0.0100\n",
      "Time taken for 1 epoch 4.698918342590332 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0055\n",
      "Epoch 58 Batch 100 Loss 0.0088\n",
      "Epoch 58 Loss 0.0083\n",
      "Time taken for 1 epoch 4.7987823486328125 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0042\n",
      "Epoch 59 Batch 100 Loss 0.0070\n",
      "Epoch 59 Loss 0.0071\n",
      "Time taken for 1 epoch 4.6475465297698975 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0060\n",
      "Epoch 60 Batch 100 Loss 0.0064\n",
      "Epoch 60 Loss 0.0060\n",
      "Time taken for 1 epoch 4.7047388553619385 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0044\n",
      "Epoch 61 Batch 100 Loss 0.0052\n",
      "Epoch 61 Loss 0.0055\n",
      "Time taken for 1 epoch 4.538601398468018 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0038\n",
      "Epoch 62 Batch 100 Loss 0.0099\n",
      "Epoch 62 Loss 0.0050\n",
      "Time taken for 1 epoch 4.702441692352295 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0030\n",
      "Epoch 63 Batch 100 Loss 0.0036\n",
      "Epoch 63 Loss 0.0046\n",
      "Time taken for 1 epoch 4.4874937534332275 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0048\n",
      "Epoch 64 Batch 100 Loss 0.0034\n",
      "Epoch 64 Loss 0.0043\n",
      "Time taken for 1 epoch 4.661391973495483 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0052\n",
      "Epoch 65 Batch 100 Loss 0.0151\n",
      "Epoch 65 Loss 0.0171\n",
      "Time taken for 1 epoch 4.572060823440552 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0365\n",
      "Epoch 66 Batch 100 Loss 0.0455\n",
      "Epoch 66 Loss 0.0488\n",
      "Time taken for 1 epoch 4.715550184249878 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0396\n",
      "Epoch 67 Batch 100 Loss 0.1100\n",
      "Epoch 67 Loss 0.0827\n",
      "Time taken for 1 epoch 4.5689826011657715 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.0546\n",
      "Epoch 68 Batch 100 Loss 0.0934\n",
      "Epoch 68 Loss 0.0947\n",
      "Time taken for 1 epoch 4.766621828079224 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0542\n",
      "Epoch 69 Batch 100 Loss 0.0724\n",
      "Epoch 69 Loss 0.0673\n",
      "Time taken for 1 epoch 4.554129362106323 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0317\n",
      "Epoch 70 Batch 100 Loss 0.0320\n",
      "Epoch 70 Loss 0.0344\n",
      "Time taken for 1 epoch 4.68841028213501 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.0180\n",
      "Epoch 71 Batch 100 Loss 0.0138\n",
      "Epoch 71 Loss 0.0143\n",
      "Time taken for 1 epoch 4.5414533615112305 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.0047\n",
      "Epoch 72 Batch 100 Loss 0.0038\n",
      "Epoch 72 Loss 0.0058\n",
      "Time taken for 1 epoch 4.712163925170898 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.0030\n",
      "Epoch 73 Batch 100 Loss 0.0033\n",
      "Epoch 73 Loss 0.0033\n",
      "Time taken for 1 epoch 4.528353452682495 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.0029\n",
      "Epoch 74 Batch 100 Loss 0.0026\n",
      "Epoch 74 Loss 0.0025\n",
      "Time taken for 1 epoch 4.658504486083984 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.0019\n",
      "Epoch 75 Batch 100 Loss 0.0025\n",
      "Epoch 75 Loss 0.0022\n",
      "Time taken for 1 epoch 4.701230525970459 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.0017\n",
      "Epoch 76 Batch 100 Loss 0.0019\n",
      "Epoch 76 Loss 0.0020\n",
      "Time taken for 1 epoch 4.86448860168457 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.0017\n",
      "Epoch 77 Batch 100 Loss 0.0019\n",
      "Epoch 77 Loss 0.0019\n",
      "Time taken for 1 epoch 5.109570026397705 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.0016\n",
      "Epoch 78 Batch 100 Loss 0.0017\n",
      "Epoch 78 Loss 0.0018\n",
      "Time taken for 1 epoch 4.963215351104736 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.0015\n",
      "Epoch 79 Batch 100 Loss 0.0016\n",
      "Epoch 79 Loss 0.0017\n",
      "Time taken for 1 epoch 4.794042110443115 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.0014\n",
      "Epoch 80 Batch 100 Loss 0.0014\n",
      "Epoch 80 Loss 0.0016\n",
      "Time taken for 1 epoch 5.049299240112305 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.0013\n",
      "Epoch 81 Batch 100 Loss 0.0016\n",
      "Epoch 81 Loss 0.0015\n",
      "Time taken for 1 epoch 5.105877876281738 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.0011\n",
      "Epoch 82 Batch 100 Loss 0.0016\n",
      "Epoch 82 Loss 0.0014\n",
      "Time taken for 1 epoch 4.946211099624634 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.0012\n",
      "Epoch 83 Batch 100 Loss 0.0015\n",
      "Epoch 83 Loss 0.0013\n",
      "Time taken for 1 epoch 4.779597759246826 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.0011\n",
      "Epoch 84 Batch 100 Loss 0.0012\n",
      "Epoch 84 Loss 0.0013\n",
      "Time taken for 1 epoch 5.040504693984985 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.0011\n",
      "Epoch 85 Batch 100 Loss 0.0011\n",
      "Epoch 85 Loss 0.0012\n",
      "Time taken for 1 epoch 4.929861068725586 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.0010\n",
      "Epoch 86 Batch 100 Loss 0.0012\n",
      "Epoch 86 Loss 0.0011\n",
      "Time taken for 1 epoch 5.070928335189819 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.0009\n",
      "Epoch 87 Batch 100 Loss 0.0010\n",
      "Epoch 87 Loss 0.0011\n",
      "Time taken for 1 epoch 4.656916618347168 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.0010\n",
      "Epoch 88 Batch 100 Loss 0.0010\n",
      "Epoch 88 Loss 0.0010\n",
      "Time taken for 1 epoch 4.716462850570679 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.0008\n",
      "Epoch 89 Batch 100 Loss 0.0009\n",
      "Epoch 89 Loss 0.0010\n",
      "Time taken for 1 epoch 4.60094428062439 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.0009\n",
      "Epoch 90 Batch 100 Loss 0.0009\n",
      "Epoch 90 Loss 0.0009\n",
      "Time taken for 1 epoch 4.645995616912842 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.0008\n",
      "Epoch 91 Batch 100 Loss 0.0009\n",
      "Epoch 91 Loss 0.0009\n",
      "Time taken for 1 epoch 4.574335098266602 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.0007\n",
      "Epoch 92 Batch 100 Loss 0.0009\n",
      "Epoch 92 Loss 0.0009\n",
      "Time taken for 1 epoch 4.691326141357422 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.0007\n",
      "Epoch 93 Batch 100 Loss 0.0007\n",
      "Epoch 93 Loss 0.0009\n",
      "Time taken for 1 epoch 4.72814679145813 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.0008\n",
      "Epoch 94 Batch 100 Loss 0.0017\n",
      "Epoch 94 Loss 0.0014\n",
      "Time taken for 1 epoch 4.828664064407349 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.0013\n",
      "Epoch 95 Batch 100 Loss 0.0337\n",
      "Epoch 95 Loss 0.0156\n",
      "Time taken for 1 epoch 4.675660133361816 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.0576\n",
      "Epoch 96 Batch 100 Loss 0.1490\n",
      "Epoch 96 Loss 0.1172\n",
      "Time taken for 1 epoch 4.820802450180054 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.0849\n",
      "Epoch 97 Batch 100 Loss 0.1572\n",
      "Epoch 97 Loss 0.1293\n",
      "Time taken for 1 epoch 5.4450178146362305 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.0607\n",
      "Epoch 98 Batch 100 Loss 0.0517\n",
      "Epoch 98 Loss 0.0573\n",
      "Time taken for 1 epoch 5.434290409088135 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.0187\n",
      "Epoch 99 Batch 100 Loss 0.0123\n",
      "Epoch 99 Loss 0.0171\n",
      "Time taken for 1 epoch 4.683709144592285 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.0054\n",
      "Epoch 100 Batch 100 Loss 0.0039\n",
      "Epoch 100 Loss 0.0049\n",
      "Time taken for 1 epoch 4.595437049865723 sec\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.0019\n",
      "Epoch 101 Batch 100 Loss 0.0019\n",
      "Epoch 101 Loss 0.0022\n",
      "Time taken for 1 epoch 4.641058921813965 sec\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.0012\n",
      "Epoch 102 Batch 100 Loss 0.0018\n",
      "Epoch 102 Loss 0.0014\n",
      "Time taken for 1 epoch 4.739299535751343 sec\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.0010\n",
      "Epoch 103 Batch 100 Loss 0.0011\n",
      "Epoch 103 Loss 0.0013\n",
      "Time taken for 1 epoch 4.640871047973633 sec\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.0009\n",
      "Epoch 104 Batch 100 Loss 0.0011\n",
      "Epoch 104 Loss 0.0012\n",
      "Time taken for 1 epoch 4.729810476303101 sec\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.0009\n",
      "Epoch 105 Batch 100 Loss 0.0009\n",
      "Epoch 105 Loss 0.0010\n",
      "Time taken for 1 epoch 4.602311372756958 sec\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.0009\n",
      "Epoch 106 Batch 100 Loss 0.0018\n",
      "Epoch 106 Loss 0.0015\n",
      "Time taken for 1 epoch 4.912609100341797 sec\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.0010\n",
      "Epoch 107 Batch 100 Loss 0.0009\n",
      "Epoch 107 Loss 0.0029\n",
      "Time taken for 1 epoch 4.508998870849609 sec\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.0014\n",
      "Epoch 108 Batch 100 Loss 0.0011\n",
      "Epoch 108 Loss 0.0018\n",
      "Time taken for 1 epoch 4.749078035354614 sec\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.0018\n",
      "Epoch 109 Batch 100 Loss 0.0011\n",
      "Epoch 109 Loss 0.0018\n",
      "Time taken for 1 epoch 5.812867164611816 sec\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.0011\n",
      "Epoch 110 Batch 100 Loss 0.0014\n",
      "Epoch 110 Loss 0.0012\n",
      "Time taken for 1 epoch 6.224364995956421 sec\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.0035\n",
      "Epoch 111 Batch 100 Loss 0.0007\n",
      "Epoch 111 Loss 0.0009\n",
      "Time taken for 1 epoch 5.893646478652954 sec\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.0006\n",
      "Epoch 112 Batch 100 Loss 0.0006\n",
      "Epoch 112 Loss 0.0009\n",
      "Time taken for 1 epoch 7.020816087722778 sec\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.0012\n",
      "Epoch 113 Batch 100 Loss 0.0005\n",
      "Epoch 113 Loss 0.0007\n",
      "Time taken for 1 epoch 6.32680869102478 sec\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.0006\n",
      "Epoch 114 Batch 100 Loss 0.0006\n",
      "Epoch 114 Loss 0.0006\n",
      "Time taken for 1 epoch 6.257211446762085 sec\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.0005\n",
      "Epoch 115 Batch 100 Loss 0.0006\n",
      "Epoch 115 Loss 0.0007\n",
      "Time taken for 1 epoch 6.229470491409302 sec\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.0005\n",
      "Epoch 116 Batch 100 Loss 0.0006\n",
      "Epoch 116 Loss 0.0006\n",
      "Time taken for 1 epoch 7.197227239608765 sec\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.0005\n",
      "Epoch 117 Batch 100 Loss 0.0005\n",
      "Epoch 117 Loss 0.0005\n",
      "Time taken for 1 epoch 5.762409925460815 sec\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.0004\n",
      "Epoch 118 Batch 100 Loss 0.0005\n",
      "Epoch 118 Loss 0.0005\n",
      "Time taken for 1 epoch 5.784593343734741 sec\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.0004\n",
      "Epoch 119 Batch 100 Loss 0.0005\n",
      "Epoch 119 Loss 0.0004\n",
      "Time taken for 1 epoch 6.5010986328125 sec\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.0004\n",
      "Epoch 120 Batch 100 Loss 0.0004\n",
      "Epoch 120 Loss 0.0004\n",
      "Time taken for 1 epoch 6.735044717788696 sec\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.0004\n",
      "Epoch 121 Batch 100 Loss 0.0004\n",
      "Epoch 121 Loss 0.0004\n",
      "Time taken for 1 epoch 5.781020402908325 sec\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.0004\n",
      "Epoch 122 Batch 100 Loss 0.0004\n",
      "Epoch 122 Loss 0.0004\n",
      "Time taken for 1 epoch 6.125390529632568 sec\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.0003\n",
      "Epoch 123 Batch 100 Loss 0.0003\n",
      "Epoch 123 Loss 0.0004\n",
      "Time taken for 1 epoch 6.132505655288696 sec\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.0003\n",
      "Epoch 124 Batch 100 Loss 0.0003\n",
      "Epoch 124 Loss 0.0003\n",
      "Time taken for 1 epoch 6.163919687271118 sec\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.0003\n",
      "Epoch 125 Batch 100 Loss 0.0004\n",
      "Epoch 125 Loss 0.0003\n",
      "Time taken for 1 epoch 6.793569803237915 sec\n",
      "\n",
      "Epoch 126 Batch 0 Loss 0.0003\n",
      "Epoch 126 Batch 100 Loss 0.0003\n",
      "Epoch 126 Loss 0.0003\n",
      "Time taken for 1 epoch 6.087540626525879 sec\n",
      "\n",
      "Epoch 127 Batch 0 Loss 0.0003\n",
      "Epoch 127 Batch 100 Loss 0.0003\n",
      "Epoch 127 Loss 0.0003\n",
      "Time taken for 1 epoch 6.224113464355469 sec\n",
      "\n",
      "Epoch 128 Batch 0 Loss 0.0003\n",
      "Epoch 128 Batch 100 Loss 0.0012\n",
      "Epoch 128 Loss 0.0013\n",
      "Time taken for 1 epoch 6.5893778800964355 sec\n",
      "\n",
      "Epoch 129 Batch 0 Loss 0.0124\n",
      "Epoch 129 Batch 100 Loss 0.0487\n",
      "Epoch 129 Loss 0.0334\n",
      "Time taken for 1 epoch 6.405973434448242 sec\n",
      "\n",
      "Epoch 130 Batch 0 Loss 0.0642\n",
      "Epoch 130 Batch 100 Loss 0.1066\n",
      "Epoch 130 Loss 0.1163\n",
      "Time taken for 1 epoch 6.341956377029419 sec\n",
      "\n",
      "Epoch 131 Batch 0 Loss 0.0594\n",
      "Epoch 131 Batch 100 Loss 0.0798\n",
      "Epoch 131 Loss 0.0758\n",
      "Time taken for 1 epoch 5.609602928161621 sec\n",
      "\n",
      "Epoch 132 Batch 0 Loss 0.0184\n",
      "Epoch 132 Batch 100 Loss 0.0335\n",
      "Epoch 132 Loss 0.0248\n",
      "Time taken for 1 epoch 6.2727766036987305 sec\n",
      "\n",
      "Epoch 133 Batch 0 Loss 0.0044\n",
      "Epoch 133 Batch 100 Loss 0.0030\n",
      "Epoch 133 Loss 0.0060\n",
      "Time taken for 1 epoch 5.802145957946777 sec\n",
      "\n",
      "Epoch 134 Batch 0 Loss 0.0016\n",
      "Epoch 134 Batch 100 Loss 0.0015\n",
      "Epoch 134 Loss 0.0018\n",
      "Time taken for 1 epoch 6.8277482986450195 sec\n",
      "\n",
      "Epoch 135 Batch 0 Loss 0.0009\n",
      "Epoch 135 Batch 100 Loss 0.0008\n",
      "Epoch 135 Loss 0.0010\n",
      "Time taken for 1 epoch 6.864264488220215 sec\n",
      "\n",
      "Epoch 136 Batch 0 Loss 0.0006\n",
      "Epoch 136 Batch 100 Loss 0.0008\n",
      "Epoch 136 Loss 0.0008\n",
      "Time taken for 1 epoch 6.506869077682495 sec\n",
      "\n",
      "Epoch 137 Batch 0 Loss 0.0006\n",
      "Epoch 137 Batch 100 Loss 0.0007\n",
      "Epoch 137 Loss 0.0007\n",
      "Time taken for 1 epoch 5.8450493812561035 sec\n",
      "\n",
      "Epoch 138 Batch 0 Loss 0.0030\n",
      "Epoch 138 Batch 100 Loss 0.0006\n",
      "Epoch 138 Loss 0.0007\n",
      "Time taken for 1 epoch 6.126164197921753 sec\n",
      "\n",
      "Epoch 139 Batch 0 Loss 0.0005\n",
      "Epoch 139 Batch 100 Loss 0.0006\n",
      "Epoch 139 Loss 0.0006\n",
      "Time taken for 1 epoch 6.580478668212891 sec\n",
      "\n",
      "Epoch 140 Batch 0 Loss 0.0012\n",
      "Epoch 140 Batch 100 Loss 0.0006\n",
      "Epoch 140 Loss 0.0005\n",
      "Time taken for 1 epoch 6.364346981048584 sec\n",
      "\n",
      "Epoch 141 Batch 0 Loss 0.0004\n",
      "Epoch 141 Batch 100 Loss 0.0005\n",
      "Epoch 141 Loss 0.0005\n",
      "Time taken for 1 epoch 6.2318501472473145 sec\n",
      "\n",
      "Epoch 142 Batch 0 Loss 0.0004\n",
      "Epoch 142 Batch 100 Loss 0.0004\n",
      "Epoch 142 Loss 0.0005\n",
      "Time taken for 1 epoch 6.812990188598633 sec\n",
      "\n",
      "Epoch 143 Batch 0 Loss 0.0004\n",
      "Epoch 143 Batch 100 Loss 0.0005\n",
      "Epoch 143 Loss 0.0011\n",
      "Time taken for 1 epoch 6.5104429721832275 sec\n",
      "\n",
      "Epoch 144 Batch 0 Loss 0.0006\n",
      "Epoch 144 Batch 100 Loss 0.0007\n",
      "Epoch 144 Loss 0.0011\n",
      "Time taken for 1 epoch 6.496218204498291 sec\n",
      "\n",
      "Epoch 145 Batch 0 Loss 0.0006\n",
      "Epoch 145 Batch 100 Loss 0.0007\n",
      "Epoch 145 Loss 0.0011\n",
      "Time taken for 1 epoch 6.6407506465911865 sec\n",
      "\n",
      "Epoch 146 Batch 0 Loss 0.0004\n",
      "Epoch 146 Batch 100 Loss 0.0100\n",
      "Epoch 146 Loss 0.0033\n",
      "Time taken for 1 epoch 6.47123122215271 sec\n",
      "\n",
      "Epoch 147 Batch 0 Loss 0.0025\n",
      "Epoch 147 Batch 100 Loss 0.0073\n",
      "Epoch 147 Loss 0.0047\n",
      "Time taken for 1 epoch 6.281412839889526 sec\n",
      "\n",
      "Epoch 148 Batch 0 Loss 0.0028\n",
      "Epoch 148 Batch 100 Loss 0.0023\n",
      "Epoch 148 Loss 0.0054\n",
      "Time taken for 1 epoch 6.460857629776001 sec\n",
      "\n",
      "Epoch 149 Batch 0 Loss 0.0012\n",
      "Epoch 149 Batch 100 Loss 0.0034\n",
      "Epoch 149 Loss 0.0039\n",
      "Time taken for 1 epoch 6.448790073394775 sec\n",
      "\n",
      "Epoch 150 Batch 0 Loss 0.0014\n",
      "Epoch 150 Batch 100 Loss 0.0074\n",
      "Epoch 150 Loss 0.0040\n",
      "Time taken for 1 epoch 5.845440149307251 sec\n",
      "\n",
      "Epoch 151 Batch 0 Loss 0.0164\n",
      "Epoch 151 Batch 100 Loss 0.0016\n",
      "Epoch 151 Loss 0.0026\n",
      "Time taken for 1 epoch 5.816831350326538 sec\n",
      "\n",
      "Epoch 152 Batch 0 Loss 0.0014\n",
      "Epoch 152 Batch 100 Loss 0.0024\n",
      "Epoch 152 Loss 0.0024\n",
      "Time taken for 1 epoch 6.324885368347168 sec\n",
      "\n",
      "Epoch 153 Batch 0 Loss 0.0026\n",
      "Epoch 153 Batch 100 Loss 0.0028\n",
      "Epoch 153 Loss 0.0028\n",
      "Time taken for 1 epoch 5.975791692733765 sec\n",
      "\n",
      "Epoch 154 Batch 0 Loss 0.0006\n",
      "Epoch 154 Batch 100 Loss 0.0017\n",
      "Epoch 154 Loss 0.0027\n",
      "Time taken for 1 epoch 7.351934909820557 sec\n",
      "\n",
      "Epoch 155 Batch 0 Loss 0.0013\n",
      "Epoch 155 Batch 100 Loss 0.0026\n",
      "Epoch 155 Loss 0.0043\n",
      "Time taken for 1 epoch 5.951026439666748 sec\n",
      "\n",
      "Epoch 156 Batch 0 Loss 0.0027\n",
      "Epoch 156 Batch 100 Loss 0.0064\n",
      "Epoch 156 Loss 0.0086\n",
      "Time taken for 1 epoch 6.46284294128418 sec\n",
      "\n",
      "Epoch 157 Batch 0 Loss 0.0080\n",
      "Epoch 157 Batch 100 Loss 0.0102\n",
      "Epoch 157 Loss 0.0132\n",
      "Time taken for 1 epoch 6.028801441192627 sec\n",
      "\n",
      "Epoch 158 Batch 0 Loss 0.0071\n",
      "Epoch 158 Batch 100 Loss 0.0173\n",
      "Epoch 158 Loss 0.0134\n",
      "Time taken for 1 epoch 6.2608301639556885 sec\n",
      "\n",
      "Epoch 159 Batch 0 Loss 0.0110\n",
      "Epoch 159 Batch 100 Loss 0.0054\n",
      "Epoch 159 Loss 0.0127\n",
      "Time taken for 1 epoch 5.990900039672852 sec\n",
      "\n",
      "Epoch 160 Batch 0 Loss 0.0087\n",
      "Epoch 160 Batch 100 Loss 0.0057\n",
      "Epoch 160 Loss 0.0102\n",
      "Time taken for 1 epoch 6.244542598724365 sec\n",
      "\n",
      "Epoch 161 Batch 0 Loss 0.0030\n",
      "Epoch 161 Batch 100 Loss 0.0049\n",
      "Epoch 161 Loss 0.0070\n",
      "Time taken for 1 epoch 6.032746315002441 sec\n",
      "\n",
      "Epoch 162 Batch 0 Loss 0.0024\n",
      "Epoch 162 Batch 100 Loss 0.0017\n",
      "Epoch 162 Loss 0.0049\n",
      "Time taken for 1 epoch 6.7760090827941895 sec\n",
      "\n",
      "Epoch 163 Batch 0 Loss 0.0005\n",
      "Epoch 163 Batch 100 Loss 0.0011\n",
      "Epoch 163 Loss 0.0031\n",
      "Time taken for 1 epoch 6.443461179733276 sec\n",
      "\n",
      "Epoch 164 Batch 0 Loss 0.0028\n",
      "Epoch 164 Batch 100 Loss 0.0009\n",
      "Epoch 164 Loss 0.0023\n",
      "Time taken for 1 epoch 6.6196510791778564 sec\n",
      "\n",
      "Epoch 165 Batch 0 Loss 0.0006\n",
      "Epoch 165 Batch 100 Loss 0.0007\n",
      "Epoch 165 Loss 0.0020\n",
      "Time taken for 1 epoch 4.696331739425659 sec\n",
      "\n",
      "Epoch 166 Batch 0 Loss 0.0004\n",
      "Epoch 166 Batch 100 Loss 0.0057\n",
      "Epoch 166 Loss 0.0014\n",
      "Time taken for 1 epoch 4.722692489624023 sec\n",
      "\n",
      "Epoch 167 Batch 0 Loss 0.0003\n",
      "Epoch 167 Batch 100 Loss 0.0004\n",
      "Epoch 167 Loss 0.0010\n",
      "Time taken for 1 epoch 4.67132043838501 sec\n",
      "\n",
      "Epoch 168 Batch 0 Loss 0.0003\n",
      "Epoch 168 Batch 100 Loss 0.0004\n",
      "Epoch 168 Loss 0.0007\n",
      "Time taken for 1 epoch 4.779956579208374 sec\n",
      "\n",
      "Epoch 169 Batch 0 Loss 0.0002\n",
      "Epoch 169 Batch 100 Loss 0.0005\n",
      "Epoch 169 Loss 0.0005\n",
      "Time taken for 1 epoch 4.675951957702637 sec\n",
      "\n",
      "Epoch 170 Batch 0 Loss 0.0002\n",
      "Epoch 170 Batch 100 Loss 0.0002\n",
      "Epoch 170 Loss 0.0004\n",
      "Time taken for 1 epoch 4.925124168395996 sec\n",
      "\n",
      "Epoch 171 Batch 0 Loss 0.0002\n",
      "Epoch 171 Batch 100 Loss 0.0002\n",
      "Epoch 171 Loss 0.0003\n",
      "Time taken for 1 epoch 4.704468011856079 sec\n",
      "\n",
      "Epoch 172 Batch 0 Loss 0.0002\n",
      "Epoch 172 Batch 100 Loss 0.0003\n",
      "Epoch 172 Loss 0.0006\n",
      "Time taken for 1 epoch 4.832779169082642 sec\n",
      "\n",
      "Epoch 173 Batch 0 Loss 0.0003\n",
      "Epoch 173 Batch 100 Loss 0.0024\n",
      "Epoch 173 Loss 0.0094\n",
      "Time taken for 1 epoch 4.652273416519165 sec\n",
      "\n",
      "Epoch 174 Batch 0 Loss 0.0040\n",
      "Epoch 174 Batch 100 Loss 0.0026\n",
      "Epoch 174 Loss 0.0127\n",
      "Time taken for 1 epoch 4.716070890426636 sec\n",
      "\n",
      "Epoch 175 Batch 0 Loss 0.0052\n",
      "Epoch 175 Batch 100 Loss 0.0067\n",
      "Epoch 175 Loss 0.0100\n",
      "Time taken for 1 epoch 4.691298723220825 sec\n",
      "\n",
      "Epoch 176 Batch 0 Loss 0.0035\n",
      "Epoch 176 Batch 100 Loss 0.0096\n",
      "Epoch 176 Loss 0.0060\n",
      "Time taken for 1 epoch 4.9019691944122314 sec\n",
      "\n",
      "Epoch 177 Batch 0 Loss 0.0017\n",
      "Epoch 177 Batch 100 Loss 0.0027\n",
      "Epoch 177 Loss 0.0038\n",
      "Time taken for 1 epoch 4.647974729537964 sec\n",
      "\n",
      "Epoch 178 Batch 0 Loss 0.0008\n",
      "Epoch 178 Batch 100 Loss 0.0059\n",
      "Epoch 178 Loss 0.0030\n",
      "Time taken for 1 epoch 4.697941064834595 sec\n",
      "\n",
      "Epoch 179 Batch 0 Loss 0.0025\n",
      "Epoch 179 Batch 100 Loss 0.0109\n",
      "Epoch 179 Loss 0.0029\n",
      "Time taken for 1 epoch 4.789180278778076 sec\n",
      "\n",
      "Epoch 180 Batch 0 Loss 0.0019\n",
      "Epoch 180 Batch 100 Loss 0.0055\n",
      "Epoch 180 Loss 0.0025\n",
      "Time taken for 1 epoch 5.218703031539917 sec\n",
      "\n",
      "Epoch 181 Batch 0 Loss 0.0029\n",
      "Epoch 181 Batch 100 Loss 0.0008\n",
      "Epoch 181 Loss 0.0023\n",
      "Time taken for 1 epoch 4.645821571350098 sec\n",
      "\n",
      "Epoch 182 Batch 0 Loss 0.0008\n",
      "Epoch 182 Batch 100 Loss 0.0035\n",
      "Epoch 182 Loss 0.0015\n",
      "Time taken for 1 epoch 4.972728967666626 sec\n",
      "\n",
      "Epoch 183 Batch 0 Loss 0.0117\n",
      "Epoch 183 Batch 100 Loss 0.0020\n",
      "Epoch 183 Loss 0.0013\n",
      "Time taken for 1 epoch 5.025120735168457 sec\n",
      "\n",
      "Epoch 184 Batch 0 Loss 0.0005\n",
      "Epoch 184 Batch 100 Loss 0.0003\n",
      "Epoch 184 Loss 0.0015\n",
      "Time taken for 1 epoch 4.795289039611816 sec\n",
      "\n",
      "Epoch 185 Batch 0 Loss 0.0095\n",
      "Epoch 185 Batch 100 Loss 0.0007\n",
      "Epoch 185 Loss 0.0015\n",
      "Time taken for 1 epoch 4.622785568237305 sec\n",
      "\n",
      "Epoch 186 Batch 0 Loss 0.0029\n",
      "Epoch 186 Batch 100 Loss 0.0009\n",
      "Epoch 186 Loss 0.0014\n",
      "Time taken for 1 epoch 4.796607494354248 sec\n",
      "\n",
      "Epoch 187 Batch 0 Loss 0.0007\n",
      "Epoch 187 Batch 100 Loss 0.0010\n",
      "Epoch 187 Loss 0.0018\n",
      "Time taken for 1 epoch 4.664530515670776 sec\n",
      "\n",
      "Epoch 188 Batch 0 Loss 0.0008\n",
      "Epoch 188 Batch 100 Loss 0.0010\n",
      "Epoch 188 Loss 0.0026\n",
      "Time taken for 1 epoch 4.879101276397705 sec\n",
      "\n",
      "Epoch 189 Batch 0 Loss 0.0006\n",
      "Epoch 189 Batch 100 Loss 0.0024\n",
      "Epoch 189 Loss 0.0036\n",
      "Time taken for 1 epoch 4.720761299133301 sec\n",
      "\n",
      "Epoch 190 Batch 0 Loss 0.0057\n",
      "Epoch 190 Batch 100 Loss 0.0055\n",
      "Epoch 190 Loss 0.0046\n",
      "Time taken for 1 epoch 4.817065477371216 sec\n",
      "\n",
      "Epoch 191 Batch 0 Loss 0.0063\n",
      "Epoch 191 Batch 100 Loss 0.0051\n",
      "Epoch 191 Loss 0.0047\n",
      "Time taken for 1 epoch 4.6601762771606445 sec\n",
      "\n",
      "Epoch 192 Batch 0 Loss 0.0011\n",
      "Epoch 192 Batch 100 Loss 0.0020\n",
      "Epoch 192 Loss 0.0042\n",
      "Time taken for 1 epoch 4.75134801864624 sec\n",
      "\n",
      "Epoch 193 Batch 0 Loss 0.0007\n",
      "Epoch 193 Batch 100 Loss 0.0015\n",
      "Epoch 193 Loss 0.0046\n",
      "Time taken for 1 epoch 4.711503744125366 sec\n",
      "\n",
      "Epoch 194 Batch 0 Loss 0.0008\n",
      "Epoch 194 Batch 100 Loss 0.0069\n",
      "Epoch 194 Loss 0.0043\n",
      "Time taken for 1 epoch 4.826146364212036 sec\n",
      "\n",
      "Epoch 195 Batch 0 Loss 0.0020\n",
      "Epoch 195 Batch 100 Loss 0.0022\n",
      "Epoch 195 Loss 0.0035\n",
      "Time taken for 1 epoch 4.581344366073608 sec\n",
      "\n",
      "Epoch 196 Batch 0 Loss 0.0025\n",
      "Epoch 196 Batch 100 Loss 0.0074\n",
      "Epoch 196 Loss 0.0051\n",
      "Time taken for 1 epoch 4.780084609985352 sec\n",
      "\n",
      "Epoch 197 Batch 0 Loss 0.0030\n",
      "Epoch 197 Batch 100 Loss 0.0029\n",
      "Epoch 197 Loss 0.0062\n",
      "Time taken for 1 epoch 4.621686935424805 sec\n",
      "\n",
      "Epoch 198 Batch 0 Loss 0.0006\n",
      "Epoch 198 Batch 100 Loss 0.0138\n",
      "Epoch 198 Loss 0.0050\n",
      "Time taken for 1 epoch 4.749517202377319 sec\n",
      "\n",
      "Epoch 199 Batch 0 Loss 0.0013\n",
      "Epoch 199 Batch 100 Loss 0.0045\n",
      "Epoch 199 Loss 0.0045\n",
      "Time taken for 1 epoch 4.706293106079102 sec\n",
      "\n",
      "Epoch 200 Batch 0 Loss 0.0036\n",
      "Epoch 200 Batch 100 Loss 0.0036\n",
      "Epoch 200 Loss 0.0042\n",
      "Time taken for 1 epoch 4.785460472106934 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f99700f51c0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: zorion\n",
      "Predicted translation: औरव <end> \n"
     ]
    }
   ],
   "source": [
    "translate('zorion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: aachaaryanandan\n",
      "Predicted translation: अर्पणा <end> \n"
     ]
    }
   ],
   "source": [
    "translate('aachaaryanandan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgblcYZIllR6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Machine Translation using LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
